{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "804jIRNThtM_",
        "outputId": "991e9f21-4ac3-4b07-9cb0-9fd7dd044533"
      },
      "source": [
        "!pip install albumentations==0.4.6\n",
        "!pip install ensemble-boxes"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: ensemble-boxes in /usr/local/lib/python3.7/dist-packages (1.0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ensemble-boxes) (1.19.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from ensemble-boxes) (0.51.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ensemble-boxes) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->ensemble-boxes) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->ensemble-boxes) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ensemble-boxes) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ensemble-boxes) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ensemble-boxes) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1Yh2ZTT_vWf",
        "outputId": "5a234c47-3f50-43bf-ef23-9476496ea252"
      },
      "source": [
        "import pathlib\n",
        "from google.colab import drive\n",
        "import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmR9OVgREFC3"
      },
      "source": [
        "# # model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "# model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-LFCFBBEHmZ"
      },
      "source": [
        "# model"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFeyIPpe9sk"
      },
      "source": [
        "# from albumentations import RandomCrop, HorizontalFlip, VerticalFlip, RandomBrightnessContrast\n",
        "import albumentations as A"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiZHXsuJN7EB"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmssVsVOFhl4"
      },
      "source": [
        "# device = torch.device('cpu')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFQsQct1Fhl4",
        "outputId": "7fcb82ce-3fee-47b6-e4f6-67e963077e64"
      },
      "source": [
        "device"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8teyMkD0r-O"
      },
      "source": [
        "root_path = pathlib.Path('/content/drive/MyDrive/anom_detection/task')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUf5PQVgTxmq"
      },
      "source": [
        "class SteelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.root = pathlib.Path(root)\n",
        "        \n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted((self.root/'images').iterdir()))\n",
        "        self.boxes = list(sorted((self.root/'anno').iterdir()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = self.root/'images'/self.imgs[idx]\n",
        "        boxes_path = self.root/'anno'/self.boxes[idx]\n",
        "        with open(boxes_path, 'r', encoding='utf-8') as g:\n",
        "            anno_file = BeautifulSoup(g.read(), 'lxml-xml')\n",
        "        img = cv2.imread(str(img_path))\n",
        "\n",
        "        # img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img /= 255.0\n",
        "        \n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        # mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        # mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        # obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        # obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        # masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        bboxes = []\n",
        "        for tags in anno_file.find_all('object'):\n",
        "            xmin = int(tags.find('bndbox').find('xmin').get_text())\n",
        "            ymin = int(tags.find('bndbox').find('ymin').get_text())\n",
        "            xmax = int(tags.find('bndbox').find('xmax').get_text())\n",
        "            ymax = int(tags.find('bndbox').find('ymax').get_text())\n",
        "            bboxes.append([xmin, ymin, xmax, ymax]) \n",
        "\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
        "        # print(boxes)\n",
        "        # print(boxes)\n",
        "        # there is only one class\n",
        "        num_objs = 1\n",
        "        # labels = torch.ones((len(self.imgs)), dtype=torch.float32)\n",
        "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
        "        # print(len(boxes))\n",
        "        # print(len(labels))\n",
        "        # print(len(img))\n",
        "        # masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        # image_id = torch.tensor([idx])\n",
        "        # area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        # iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        # target[\"masks\"] = masks\n",
        "        # target[\"image_id\"] = image_id\n",
        "        # target[\"area\"] = area\n",
        "        # target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "\n",
        "          \n",
        "            # print('fdsfsdf')\n",
        "            # if len(sample['bboxes']) > 0:\n",
        "            #         image = sample['image']\n",
        "            #         target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes']))))\n",
        "                     \n",
        "            # img = self.transforms(image=img)[\"image\"]\n",
        "\n",
        "        # boxes = torch.stack(tuple(map(torch.tensor, target['boxes'])))\n",
        "        # data_dict = {'image' : img, 'boxes' : boxes}\n",
        "        # return data_dict\n",
        "        # print(img.shape)\n",
        "        # print(len(boxes))\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66vbKQQV7DwK"
      },
      "source": [
        "class AbstractDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.subset[index]\n",
        "        if self.transform:\n",
        "            sample = self.transform(**{\n",
        "                    'image': img,\n",
        "                    'bboxes': target['boxes'],\n",
        "                    'labels': target['labels']\n",
        "                      })\n",
        "            img = sample['image']\n",
        "            target['boxes'] = torch.as_tensor(sample['bboxes'], dtype=torch.float32)            \n",
        "            target['labels'] = torch.as_tensor(sample['labels'], dtype=torch.int64)\n",
        "           \n",
        "        return img, target\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.subset)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2S2gPnmR1sv"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.utils.data as utils\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import time"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPDycUITyVjm"
      },
      "source": [
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True, trainable_backbone_layers=3)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6MckcD-h6zp"
      },
      "source": [
        "from albumentations.pytorch import ToTensorV2"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FikOipxS-wPF"
      },
      "source": [
        "# Randomcrop, Hflip, Vflip, RandomBrightnessContrast\n",
        "\n",
        "# trans = transforms.Compose([\n",
        "#             transforms.ToTensor(),      \n",
        "#             transforms.Resize((224, 224)),\n",
        "#             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                      std=[0.229, 0.224, 0.225])                    \n",
        "#         ])\n",
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "                      # A.RandomCrop(height=150, width=150, p=0.5),\n",
        "                      A.RandomSizedBBoxSafeCrop(height=150, width=150, p=0.5),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
        "                                           contrast_limit=0.2, p=0.9),\n",
        "            A.HorizontalFlip(p=0.3),\n",
        "            A.VerticalFlip(p=0.3),\n",
        "            A.Resize(224, 224, interpolation=1),\n",
        "            # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(p=1.0)\n",
        "        ], \n",
        "        bbox_params=A.BboxParams(\n",
        "            format='pascal_voc',\n",
        "            min_area=0, \n",
        "            min_visibility=0,\n",
        "            label_fields=['labels']\n",
        "        ))\n",
        "    \n",
        "def get_test_transforms():\n",
        "    return A.Compose([\n",
        "            A.Resize(224, 224, interpolation=1),\n",
        "            # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(p=1.0)\n",
        "        ], \n",
        "        bbox_params=A.BboxParams(\n",
        "            format='pascal_voc',\n",
        "            min_area=0, \n",
        "            min_visibility=0,\n",
        "            label_fields=['labels']\n",
        "        ))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzC29O5Uv3oC"
      },
      "source": [
        "dataset = SteelDataset(root_path)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.2)])"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTENuMB68cyi"
      },
      "source": [
        "train_data = AbstractDataset(\n",
        "    train_set, transform=get_train_transforms()\n",
        ")\n",
        "test_data = AbstractDataset(\n",
        "    test_set, \n",
        "    transform=get_test_transforms()\n",
        ")"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6mC_K4VztI_"
      },
      "source": [
        "model.to(device)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-5, weight_decay=1e-3)\n",
        "lr_scheduler = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1p6CwBYSS9j"
      },
      "source": [
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59zC2GcmQCV6"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
        "    This describes how to combine these tensors of different sizes. We use lists.\n",
        "    Note: this need not be defined in this Class, can be standalone.\n",
        "    :param batch: an iterable of N sets from __getitem__()\n",
        "    :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
        "    \"\"\"\n",
        "\n",
        "    # return images, target\n",
        "    return tuple(zip(*batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Fg3Q4o0Req"
      },
      "source": [
        "# from itertools import zip_longest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-o5hPfvN8UH"
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, \n",
        "                                               num_workers=2, \n",
        "                                               collate_fn=collate_fn, \n",
        "                                               drop_last=True\n",
        "                                               )\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True, # shuffle for test\n",
        "                                               num_workers=2, \n",
        "                                              collate_fn=collate_fn,\n",
        "                                              drop_last=True\n",
        "                                              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-T_uqI2a1BH"
      },
      "source": [
        "num_epochs = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4c3zoQIz88e"
      },
      "source": [
        "loss_hist_train = Averager()\n",
        "loss_hist_test = Averager()\n",
        "# itr = 1\n",
        "\n",
        "def train_epoch(images, targets):\n",
        "    model.train()\n",
        "    images = torch.stack(images)\n",
        "    images = images.to(device).float()\n",
        "    # targets = {}\n",
        "    # targets['boxes'] = boxes\n",
        "    # targets['labels'] = labels\n",
        "    # print(targets)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    # print(targets)\n",
        "    loss_dict = model(images, targets)\n",
        "    # print(loss_dict)\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "    loss_value = losses.item()\n",
        "\n",
        "    loss_hist_train.send(loss_value)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    losses.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "def test_epoch(images, targets):\n",
        "    model.eval()\n",
        "    summary_loss = Averager()\n",
        "    images = torch.stack(images)\n",
        "    images = images.to(device).float()\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    loss_dict = model(images, targets)\n",
        "    # print(loss_dict)\n",
        "    # losses = sum(loss for loss in loss_dict.values())\n",
        "    loss_value = losses.item()\n",
        "\n",
        "    loss_hist_test.send(loss_value)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss_hist_train.reset()\n",
        "    loss_hist_test.reset()\n",
        "    \n",
        "    for images, targets in train_dataloader:\n",
        "        train_epoch(images, targets)\n",
        "        # print(images[0])\n",
        "        # images = list(image.to(device) for image in images)\n",
        "    # for images, targets in test_dataloader:\n",
        "    #     with torch.no_grad():\n",
        "    #         test_epoch(images, targets)\n",
        "\n",
        "\n",
        "        # if itr % 50 == 0:\n",
        "        #     print(f\"Iteration #{itr} loss: {loss_value}\")\n",
        "\n",
        "        # itr += 1\n",
        "    \n",
        "    # update the learning rate\n",
        "\n",
        "\n",
        "    print(f\"Epoch #{epoch} train loss: {loss_hist_train.value}\")\n",
        "    # print(f\"Epoch #{epoch} test loss: {loss_hist_test.value}\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt4B0slXqnza"
      },
      "source": [
        "# def get_val_transforms():\n",
        "#     return A.Compose([\n",
        "#             A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "#             ToTensorV2(p=1.0)\n",
        "#         ], )"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-90rDPkN1un"
      },
      "source": [
        "# test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=8, shuffle=False, \n",
        "#                                                num_workers=2, \n",
        "#                                               collate_fn=collate_fn,\n",
        "#                                               drop_last=True\n",
        "#                                               )\n",
        "\n",
        "# dataset_check = SteelDataset('/content/drive/MyDrive/anom_detection/task', augmentations=None)\n",
        "# train_set_check, test_set_check = torch.utils.data.random_split(dataset_check, [int(len(dataset)*0.8), int(len(dataset)*0.2)])\n",
        "# test_dataloader_check = torch.utils.data.DataLoader(test_set_check, batch_size=8, shuffle=True, \n",
        "#                                                num_workers=2, \n",
        "#                                                collate_fn=collate_fn\n",
        "#                                                )"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9FHnqXtVXsD"
      },
      "source": [
        "model.eval()\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "model.to(cpu_device)\n",
        "sample = images[0].cpu().numpy()\n",
        "sample = (transforms.ToTensor()(sample)).permute(1,2,0).unsqueeze(0)\n",
        "outputs = model(sample)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhCD0CuYWub8",
        "outputId": "6f624b2b-8318-4d17-bdde-0c09d9f9462e"
      },
      "source": [
        "outputs"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[ 10.2645, 105.6953,  35.7524, 197.9540],\n",
              "          [  5.2397,   8.1229,  25.6115,  80.1685],\n",
              "          [ 88.3357,  87.6035, 116.9236, 206.3392],\n",
              "          [136.1001,  83.0471, 159.4272, 143.8305],\n",
              "          [157.8567,   5.2171, 178.2957,  49.8456],\n",
              "          [148.9407,   3.9425, 181.0681, 117.1028],\n",
              "          [154.4190,  52.9037, 179.4691,  93.0000],\n",
              "          [133.1823,  38.9374, 177.7421, 145.5850],\n",
              "          [133.8241,  74.2984, 165.7640, 173.6457],\n",
              "          [  4.8298,  23.8083,  33.7125, 198.6281]], grad_fn=<StackBackward>),\n",
              "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'scores': tensor([0.9762, 0.9632, 0.9422, 0.8786, 0.6044, 0.3774, 0.2202, 0.1339, 0.1252,\n",
              "          0.0573], grad_fn=<IndexBackward>)}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztk0vbkuXa6D",
        "outputId": "119927c4-67c8-4d8c-9caf-ed4bc7d181d0"
      },
      "source": [
        "outputs[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': tensor([[ 10.2645, 105.6953,  35.7524, 197.9540],\n",
              "         [  5.2397,   8.1229,  25.6115,  80.1685],\n",
              "         [ 88.3357,  87.6035, 116.9236, 206.3392],\n",
              "         [136.1001,  83.0471, 159.4272, 143.8305],\n",
              "         [157.8567,   5.2171, 178.2957,  49.8456],\n",
              "         [148.9407,   3.9425, 181.0681, 117.1028],\n",
              "         [154.4190,  52.9037, 179.4691,  93.0000],\n",
              "         [133.1823,  38.9374, 177.7421, 145.5850],\n",
              "         [133.8241,  74.2984, 165.7640, 173.6457],\n",
              "         [  4.8298,  23.8083,  33.7125, 198.6281]], grad_fn=<StackBackward>),\n",
              " 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'scores': tensor([0.9762, 0.9632, 0.9422, 0.8786, 0.6044, 0.3774, 0.2202, 0.1339, 0.1252,\n",
              "         0.0573], grad_fn=<IndexBackward>)}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om0wQgweXAm_"
      },
      "source": [
        "# from ensemble_boxes import *\n",
        "# iou_thr = 0.5\n",
        "# skip_box_thr = 0.0001\n",
        "# sigma = 0.1\n",
        "# for i in range(8):\n",
        "#     boxes = [output['boxes'].data.cpu().numpy()/(224-1) for output in outputs]\n",
        "#     scores = [output['scores'].data.cpu().numpy() for output in outputs]\n",
        "#     labels = [np.ones(output['scores'].shape[0]) for output in outputs]\n",
        "#     boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDh9YDM2jZJm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from ensemble_boxes import *"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKivM-yXdfeI"
      },
      "source": [
        "iou_thr = 0.1\n",
        "skip_box_thr = 0.4\n",
        "# sigma = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH08b_7mbOC7"
      },
      "source": [
        "model.eval()\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "model.to(cpu_device)\n",
        "batch_size = 8\n",
        "\n",
        "for counter_to_stop, (images, targets) in enumerate(test_dataloader):\n",
        "    counter = counter_to_stop * batch_size\n",
        "    # print(counter)\n",
        "    if counter < 20:\n",
        "      for len_sample in range(batch_size):\n",
        "          boxes_true = targets[len_sample]['boxes'].cpu()\n",
        "          # print(boxes_true)\n",
        "          sample = images[len_sample].cpu().numpy()\n",
        "          sample = (transforms.ToTensor()(sample)).permute(1,2,0).unsqueeze(0)\n",
        "          outputs = model(sample)\n",
        "          outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "          boxes = [output['boxes'].data.cpu().numpy()/(224-1) for output in outputs]\n",
        "          scores = [output['scores'].data.cpu().numpy() for output in outputs]\n",
        "          labels = [np.ones(output['scores'].shape[0]) for output in outputs]\n",
        "          boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
        "          boxes = boxes*(224-1)\n",
        "\n",
        "          fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "\n",
        "          # for box in boxes:\n",
        "          #     cv2.rectangle(sample,\n",
        "          #                   (box[0], box[1]),\n",
        "          #                   (box[2], box[3]),\n",
        "          #                   (220, 0, 0), 3)\n",
        "\n",
        "          for box_ind in range(len(boxes)):\n",
        "              x_min,y_min,x_max,y_max = boxes[box_ind]\n",
        "              rect_pred = patches.Rectangle((x_min,y_min), (x_max-x_min),(y_max-y_min),  linewidth=1, edgecolor='r', facecolor='none' )\n",
        "              ax.add_patch(rect_pred)\n",
        "\n",
        "          for box_ind_true in range(len(boxes_true)):\n",
        "              x_min,y_min,x_max,y_max = boxes_true[box_ind_true]\n",
        "              rect_pred = patches.Rectangle((x_min,y_min), (x_max-x_min),(y_max-y_min),  linewidth=1, edgecolor='b', facecolor='none' )\n",
        "              ax.add_patch(rect_pred)\n",
        "              \n",
        "          ax.set_axis_off()\n",
        "          # print(images[len_sample].shape)\n",
        "          ax.imshow(images[len_sample].permute(1,2,0))\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2Is-R_ai5G7"
      },
      "source": [
        "# model.eval()\n",
        "# cpu_device = torch.device(\"cpu\")\n",
        "# model.to(cpu_device)\n",
        "# batch_size = 8\n",
        "\n",
        "# for counter_to_stop, (images, targets) in enumerate(test_dataloader):\n",
        "#     counter = counter_to_stop * batch_size\n",
        "#     print(counter)\n",
        "#     if counter < 20:\n",
        "#       for len_sample in range(batch_size):\n",
        "#           boxes = targets[len_sample]['boxes'].cpu()\n",
        "#           sample = images[len_sample].cpu().numpy()\n",
        "#           sample = (transforms.ToTensor()(sample)).permute(1,2,0).unsqueeze(0)\n",
        "#           outputs = model(sample)\n",
        "#           outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "\n",
        "\n",
        "\n",
        "#           fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "\n",
        "#           # for box in boxes:\n",
        "#           #     cv2.rectangle(sample,\n",
        "#           #                   (box[0], box[1]),\n",
        "#           #                   (box[2], box[3]),\n",
        "#           #                   (220, 0, 0), 3)\n",
        "\n",
        "#           for box_ind in range(len(outputs[0]['boxes'])):\n",
        "#               x_min,y_min,x_max,y_max = outputs[0]['boxes'][box_ind]\n",
        "#               rect = patches.Rectangle((x_min,y_min), (x_max-x_min),(y_max-y_min),  linewidth=1, edgecolor='r', facecolor='none' )\n",
        "#               ax.add_patch(rect)\n",
        "              \n",
        "#           ax.set_axis_off()\n",
        "#           ax.imshow(images[len_sample].permute(1,2,0))\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szIHOXqdNLOG"
      },
      "source": [
        "# images, targets = next(iter(test_dataloader))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHfeRuQgNWjJ"
      },
      "source": [
        "# boxes = targets[5]['boxes'].cpu()\n",
        "# sample = images[5].cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGpkgPNwNaJl"
      },
      "source": [
        "# model.eval()\n",
        "# cpu_device = torch.device(\"cpu\")\n",
        "# model.to(cpu_device)\n",
        "\n",
        "# sample = (transforms.ToTensor()(sample)).permute(1,2,0).unsqueeze(0)\n",
        "# outputs = model(sample)\n",
        "# outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQZ9HG9USd-q"
      },
      "source": [
        "# outputs[0]['boxes']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U6ZWIfWtZLp"
      },
      "source": [
        "# sample.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydclREb9NdFW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "\n",
        "# for box in boxes:\n",
        "#     cv2.rectangle(sample,\n",
        "#                   (box[0], box[1]),\n",
        "#                   (box[2], box[3]),\n",
        "#                   (220, 0, 0), 3)\n",
        "\n",
        "for box_ind in range(len(outputs[0]['boxes'])):\n",
        "    x_min,y_min,x_max,y_max = outputs[0]['boxes'][box_ind]\n",
        "    rect = patches.Rectangle((x_min,y_min), (x_max-x_min),(y_max-y_min),  linewidth=1, edgecolor='r', facecolor='none' )\n",
        "    ax.add_patch(rect)\n",
        "    \n",
        "ax.set_axis_off()\n",
        "ax.imshow(images[5].permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18QrTuzH6mS8"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.patches as patches\n",
        "\n",
        "# dataset_check = SteelDataset('/content/drive/MyDrive/anom_detection/task', augmentations=None)\n",
        "# train_set_check, test_set_check = torch.utils.data.random_split(dataset_check, [int(len(dataset)*0.8), int(len(dataset)*0.2)])\n",
        "# train_dataloader_check = torch.utils.data.DataLoader(train_set_check, batch_size=16, shuffle=True, \n",
        "#                                                num_workers=2, \n",
        "#                                                collate_fn=collate_fn\n",
        "#                                                )\n",
        "\n",
        "# train_features, train_labels = next(iter(train_dataloader_check))\n",
        "\n",
        "# for sample in range(len(train_features)):\n",
        "#   img = transforms.functional.pil_to_tensor(train_features[sample]).permute(1,2,0)\n",
        "#   boxes = train_labels[sample]\n",
        "\n",
        "#   fig, ax = plt.subplots()\n",
        "#   ax.imshow(img)\n",
        "\n",
        "#   # Create a bboxes\n",
        "#   for box_ind in range(len(boxes)):\n",
        "#     x_min,y_min,x_max,y_max = boxes[box_ind]\n",
        "#     rect = patches.Rectangle((x_min,y_min), (x_max-x_min),(y_max-y_min),  linewidth=1, edgecolor='r', facecolor='none' )\n",
        "#     ax.add_patch(rect)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OCD7ZcWSa6y"
      },
      "source": [
        ""
      ],
      "execution_count": 58,
      "outputs": []
    }
  ]
}